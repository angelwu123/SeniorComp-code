import time
from selenium import webdriver
import re
from selenium.webdriver.common.by import By
from openpyxl import load_workbook
from pyquery import PyQuery as pq
import datetime


browser = webdriver.Chrome()


INDEX_URL = 'https://www.agoda.cn/search?city={number}&checkIn={check_in}&los=6&rooms=1&adults=1&children=0&cid=1846336&locale=zh-hk&ckuid=0f81fed4-7497-46c6-a219-a4ca91e9207d&prid=0&currency=USD&correlationId=33e938c4-7df9-4271-97e5-528178665149&pageTypeId=103&realLanguageId=7&languageId=7&origin=CN&tag=e8eb6bf0-38a7-11ed-87c3-6f6dd1d1194b&userId=0f81fed4-7497-46c6-a219-a4ca91e9207d&whitelabelid=1&loginLvl=0&storefrontId=3&currencyId=7&currencyCode=USD&htmlLanguage=zh-hk&cultureInfoName=zh-hk&machineName=user-7d4d4c65cc-j6f86&trafficGroupId=7&sessionId=gn3pl5fyevgfedyczp03inji&trafficSubGroupId=202&aid=266742&useFullPageLogin=true&cttp=4&isRealUser=true&mode=production&checkOut={check_out}&priceCur=USD&textToSearch={city1}&travellerType=0&familyMode=off&productType=-1'
def scrape_url(url):
    browser.get(url)
    #waits maximum 10 seconds for element's presence
    browser.implicitly_wait(0)

def parse_url(html):
    #<q>
    doc = pq(html)
    items = doc('.PropertyCard__Container')
    datas = []
    for i in items.items():
        data = []
        #Hotelname class =
        name = i('.PropertyCard__HotelName').text()
        #price
        price = i('.PropertyCardPrice__Value').text()
        #rating
        content = i('.Hkrzy').text()
        city = browser.find_element(By.CSS_SELECTOR,'.Address__Text').text
        data.append(name)
        data.append(price)
        data.append(content)
        data.append(city)
        datas.append(data)
    return datas


def main():
    day = datetime.date.today()
    list1 = ['Los Angeles', 'New York', 'Chicago', 'Houston', 'Phoenix']

    dict = {'Houston': 1178, 'New York': 318, 'Los Angeles': 12772, 'Chicago': 13899,
            'Phoenix': 10461}

    pattern =  re.compile('Page 1 of (.*)')
    for i in list1:
        wb = load_workbook(f'{i}.xlsx')
        ws = wb.active
        h = 1
        for s in range(0, 8):
            d = 1
            in_date = day + datetime.timedelta(days=s)
            out_date = in_date + datetime.timedelta(days=7)
            ws.cell(d, h).value = in_date
            ws.cell(d + 1, h).value = 'name'
            ws.cell(d + 1, h + 1).value = 'price'
            ws.cell(d + 1, h + 2).value = 'content'
            ws.cell(d + 1, h + 3).value = 'city'
            ws.cell(d + 1, h + 4).value = 'photo'
            d += 2
            scrape_url(
                INDEX_URL.format(city1=i,  check_in=in_date, check_out=out_date, number=dict[i]))
            browser.execute_script("window.scrollTo(0,document.body.scrollHeight)")
            time.sleep(3)
            browser.execute_script("window.scrollTo(0,document.body.scrollHeight)")
            time.sleep(3)
            browser.implicitly_wait(10)
            pages = browser.find_element(By.CSS_SELECTOR, '.pagination2 .pagination2__text').text
            pages = re.findall(pattern,pages)[0]
            try:
                for p in range(0, int(pages)):
                    browser.execute_script("window.scrollTo(0,5000)")
                    time.sleep(4)
                    browser.execute_script("window.scrollTo(500,10000)")
                    time.sleep(4)
                    browser.execute_script("window.scrollTo(0,15000)")
                    time.sleep(4)
                    browser.execute_script("window.scrollTo(0,20000)")
                    time.sleep(4)
                    browser.execute_script("window.scrollTo(0,25000)")
                    time.sleep(2)
                    browser.implicitly_wait(10)
                    datas = parse_url(browser.page_source)
                    for data in datas:
                        ws.cell(d, h).value = data[0]
                        ws.cell(d, h + 1).value = data[1]
                        ws.cell(d, h + 2).value = data[2]
                        ws.cell(d, h + 3).value = data[3]
                        ws.cell(d, h + 4).value = data[4]
                        d += 1
                    wb.save(f'{i}.xlsx')
                    browser.find_elements(By.CSS_SELECTOR, '.pagination2 .btn')[-1].click()
                    time.sleep(5)
            except:
                pass
            h += 5

if __name__=='__main__':
    main()
    browser.close()

